{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Searching_1-6.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNgf8fT1tv6vlziR6lUQcNo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EZy9tNAADOOy","colab_type":"code","outputId":"be5e15f8-ed0d-4806-8d20-18df770722ac","executionInfo":{"status":"ok","timestamp":1586545576499,"user_tz":-120,"elapsed":725,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2b8-GO13Dj7e","colab_type":"code","outputId":"4374906c-14c1-4da3-814e-59bec2be3814","executionInfo":{"status":"ok","timestamp":1586545583049,"user_tz":-120,"elapsed":7244,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["import tensorflow as tf\n","from tensorflow import keras as ks\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix\n","from skimage.transform import resize\n","\n","import cv2\n","\n","#Data visualization\n","import seaborn as sns\n","\n","from matplotlib import pyplot as plt\n","\n","import glob\n","import os, os.path\n","\n","#Per modello NN\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import CuDNNLSTM\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import ConvLSTM2D\n","from tensorflow.keras.layers import MaxPool2D\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Softmax\n","from tensorflow.keras.layers import Dropout\n","\n","import time\n","import math\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.15.2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sm6tDFNZDkzU","colab_type":"code","outputId":"4c60e38e-5864-459c-9fbd-d5feb77a0874","executionInfo":{"status":"ok","timestamp":1586545583052,"user_tz":-120,"elapsed":7215,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["class Stats:\n","\n","  def __init__( self, img_dim, perc_used, batch_size_used, \n","                val_acc, val_loss, total_epochs,\n","                early_stopping_epochs, total_time, model):\n","\n","    self.img_dim = img_dim           \n","    self.perc = str(perc_used)\n","    self.batch_size = str(batch_size_used)\n","    \n","    self.val_acc = str(round(val_acc, 5))\n","    self.val_loss = str(round(val_loss, 5))\n","\n","    self.total_epochs = str(total_epochs)\n","    self.early_stopping_epochs = str(early_stopping_epochs)\n","    self.training_time = str(round(total_time, 5))\n","    self.model = model\n","\n","  def myStats(self):\n","    print(\"Dimensione immagini: \", self.img_dim)\n","    print(\"Percentuale test set: \" + self.perc + \"%\")\n","    print(\"Dimensione batch size: \" + self.batch_size)\n","\n","    print(\"Val accuracy: \" + self.val_acc)\n","    print(\"Val loss: \" + self.val_loss)\n","\n","    print(\"Epoche di addestramento utilizzate: \" + self.early_stopping_epochs + \"/\" + self.total_epochs)\n","    print(\"Tempo di addestramento: \" + self.training_time + \" sec\")\n","    print(\"Model: \" + self.model)\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ahkZ92zCDlnj","colab_type":"code","outputId":"a229c1d9-bf1a-4e17-df93-8c64f3904bf3","executionInfo":{"status":"ok","timestamp":1586545583058,"user_tz":-120,"elapsed":7180,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#Extract the class from the file name, if the class is the string before che -\n","def extract_label(from_string):\n","  position = from_string.index('-') # gets position of the - in the filename\n","  substring = from_string[0:position]\n","  return substring\n","\n","def extract_obf(from_string):\n","  start_pos = from_string.index('-')\n","  end_pos = from_string.index('.')\n","  substring = from_string[(start_pos + 1):end_pos]\n","  return substring\n","\n","def mapping_labels_encoded(label_encoder):\n","  for index in range(len(list(label_encoder.classes_))):\n","    print(index, end = \"-> \")\n","    print(list(label_encoder.inverse_transform([index]))) \n","\n","class TimeHistory(ks.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        self.times = []\n","\n","    def on_epoch_begin(self, epoch, logs={}):\n","        self.epoch_time_start = time.time()\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        self.times.append(time.time() - self.epoch_time_start)\n","\n","def plot_image(i, predictions_array, true_label, img):\n","  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","\n","  plt.imshow(img, cmap=plt.cm.binary)\n","\n","  predicted_label = np.argmax(predictions_array)\n","  if predicted_label == true_label:\n","    color = 'blue'\n","  else:\n","    color = 'red'\n","\n","  plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n","                                100*np.max(predictions_array),\n","                                true_label),\n","                                color=color)\n","\n","def plot_value_array(i, predictions_array, true_label):\n","  predictions_array, true_label = predictions_array, true_label[i]\n","  plt.grid(False)\n","  plt.xticks(range(10))\n","  plt.yticks([])\n","  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n","  plt.ylim([0, 1])\n","  predicted_label = np.argmax(predictions_array)\n","\n","  thisplot[predicted_label].set_color('red')\n","  thisplot[true_label].set_color('blue')\n","\n","\n","print(\"Done\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dRqO1ku7Dnve","colab_type":"code","outputId":"4302a210-5c47-4b43-c183-3032c63b73f2","executionInfo":{"status":"ok","timestamp":1586545638747,"user_tz":-120,"elapsed":62844,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["! git clone https://github.com/PaoloDarienzo/DB_Repo.git"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'DB_Repo'...\n","remote: Enumerating objects: 8293, done.\u001b[K\n","remote: Total 8293 (delta 0), reused 0 (delta 0), pack-reused 8293\u001b[K\n","Receiving objects: 100% (8293/8293), 124.99 MiB | 18.88 MiB/s, done.\n","Resolving deltas: 100% (6795/6795), done.\n","Checking out files: 100% (14100/14100), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZzaH20_FDo7O","colab_type":"code","colab":{}},"source":["path, dirs, files = next(os.walk(\"/content/DB_Repo\"))\n","file_count = len(files)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZfEDP61Dptk","colab_type":"code","outputId":"376e73b6-7906-421e-c570-e293a2c4bdda","executionInfo":{"status":"ok","timestamp":1586545645918,"user_tz":-120,"elapsed":69972,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["%cd /content/DB_Repo\n","!pwd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/DB_Repo\n","/content/DB_Repo\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SJFDjMTeOKL6","colab_type":"text"},"source":["#INIZIO CODICI NN"]},{"cell_type":"code","metadata":{"id":"hm231XqaLPdA","colab_type":"code","colab":{}},"source":["def codice_1():\n","\n","  batch_size = 512\n","\n","  time_steps = 64\n","  n_features = 64\n","  #size_ts_blocks = 8\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 64\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","  modelLSTM.add(CuDNNLSTM(num_units1, input_shape=(time_steps, n_features), unit_forget_bias='true'))\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model1\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sp6t0aCZqqHN","colab_type":"code","colab":{}},"source":["def codice_2():\n","\n","  batch_size = 256\n","\n","  time_steps = 256\n","  n_features = 64\n","  #size_ts_blocks = 8\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 256\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","  modelLSTM.add(CuDNNLSTM(num_units1, input_shape=(time_steps, n_features), unit_forget_bias='true'))\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model1\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9Jiozezqxyj","colab_type":"code","colab":{}},"source":["def codice_3():\n","\n","  batch_size = 512\n","\n","  time_steps = 64\n","  n_features = 64\n","  #size_ts_blocks = 8\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 64\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","\n","  modelLSTM.add(Bidirectional(CuDNNLSTM(num_units1, unit_forget_bias='true'),\n","                              input_shape=(time_steps, n_features)))\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model1_bi\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NNhlniTrTRw","colab_type":"code","colab":{}},"source":["def codice_4():\n","\n","  batch_size = 256\n","\n","  time_steps = 256\n","  n_features = 64\n","  #size_ts_blocks = 8\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 256\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","\n","  modelLSTM.add(Bidirectional(CuDNNLSTM(num_units1, unit_forget_bias='true'),\n","                              input_shape=(time_steps, n_features)))\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model1_bi\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7bWCcleragi","colab_type":"code","colab":{}},"source":["def codice_5():\n","\n","  batch_size = 512\n","\n","  time_steps = 8\n","  n_features = 8\n","  size_ts_blocks = 8\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 64\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","  n_img, dim1, dim2 = train_images.shape\n","  n_img2, dim12, dim22 = test_images.shape\n","\n","  train_images = train_images.reshape(n_img, time_steps, n_features, -1, size_ts_blocks)\n","\n","  test_images = test_images.reshape(n_img2, time_steps, n_features, -1, size_ts_blocks)\n","\n","  _, _, _, val_derivato, _ = train_images.shape \n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","\n","  modelLSTM.add(ConvLSTM2D(num_units1, (3, 3), input_shape=(time_steps, n_features, val_derivato, size_ts_blocks), padding='same', unit_forget_bias='true', activation='relu'))\n","  modelLSTM.add(Flatten())\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model2\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O2RJafTBsvIY","colab_type":"code","colab":{}},"source":["def codice_6():\n","\n","  batch_size = 256\n","\n","  time_steps = 8\n","  n_features = 16\n","  size_ts_blocks = 16\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 256\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","  n_img, dim1, dim2 = train_images.shape\n","  n_img2, dim12, dim22 = test_images.shape\n","\n","  train_images = train_images.reshape(n_img, time_steps, n_features, -1, size_ts_blocks)\n","\n","  test_images = test_images.reshape(n_img2, time_steps, n_features, -1, size_ts_blocks)\n","\n","  _, _, _, val_derivato, _ = train_images.shape \n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","\n","  modelLSTM.add(ConvLSTM2D(num_units1, (3, 3), input_shape=(time_steps, n_features, val_derivato, size_ts_blocks), padding='same', unit_forget_bias='true', activation='relu'))\n","  modelLSTM.add(Flatten())\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model2\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"teRtikYNrOm1","colab_type":"code","colab":{}},"source":["def codice_7():\n","\n","  batch_size = 512\n","\n","  time_steps = 8\n","  n_features = 8\n","  size_ts_blocks = 8\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 64\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","  n_img, dim1, dim2 = train_images.shape\n","  n_img2, dim12, dim22 = test_images.shape\n","\n","  train_images = train_images.reshape(n_img, time_steps, n_features, -1, size_ts_blocks)\n","\n","  test_images = test_images.reshape(n_img2, time_steps, n_features, -1, size_ts_blocks)\n","\n","  _, _, _, val_derivato, _ = train_images.shape \n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","\n","  modelLSTM.add(Bidirectional(ConvLSTM2D(num_units1, (3, 3),\n","                              padding='same', unit_forget_bias='true', activation='relu'), \n","                              input_shape=(time_steps, n_features, val_derivato, size_ts_blocks)))\n","  modelLSTM.add(Flatten())\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model2_bi\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u2ZFZvudtYds","colab_type":"code","colab":{}},"source":["def codice_8():\n","\n","  batch_size = 256\n","\n","  time_steps = 8\n","  n_features = 16\n","  size_ts_blocks = 16\n","\n","  #Unit in first layer\n","  num_units1 = 141\n","\n","  new_dim = 256\n","  MAX_LEN = 64 #fisso\n","  channels = 1\n","\n","  n_epochs = 100\n","\n","  #Considero il primo 20% della lista di dati come test set\n","  percentage_required = 20 #%\n","\n","  #COSTANTI E DICHIARAZIONI\n","\n","  database_list = list()\n","  labels_list = list()\n","  obf_list = list()\n","\n","  #LETTURA E RESIZE IMMAGINI\n","\n","  print(\"START IMAGE INPUT\")\n","  #Aggiungo i valori alle liste leggendo i vari files\n","  for filename in glob.glob('*.npy'):\n","    temp_img = np.load(filename)\n","    temp_img = temp_img.reshape((-1, MAX_LEN)).astype('float32') \n","    temp_img = cv2.resize(temp_img, (MAX_LEN, new_dim), interpolation=cv2.INTER_CUBIC)\n","    database_list.append(temp_img)\n","    #Salvo la label, ossia la classe\n","    labels_list.append(extract_label(filename))\n","    #Salvo la lista di offuscatori di ogni file\n","    obf_list.append(extract_obf(filename))\n","  print(\"END IMAGE INPUT\")\n","\n","\n","  #SHUFFLE\n","\n","  #Ho i valori e le etichette in due liste (+ obf); \n","  #le mescolo mantenendo l'ordine tra valore-label\n","  temp = list(zip(database_list, labels_list, obf_list))\n","  np.random.shuffle(temp)\n","  database_list, labels_list, obf_list = zip(*temp)\n","\n","\n","  #SUDDIVISIONE DATI\n","\n","  #Suddivido in training set e test set\n","  assert len(database_list) == len(labels_list) == len(obf_list)\n","\n","  index_to_split = math.ceil((len(database_list) * percentage_required) / 100)\n","  indices = [(0, index_to_split - 1), (index_to_split, len(database_list) - 1)]\n","\n","  test_list, training_list = [database_list[s:e+1] for s,e in indices]\n","  labels_test_list, labels_training_list = [labels_list[s:e+1] for s,e in indices]\n","  obf_test_list, obf_training_list = [obf_list[s:e+1] for s,e in indices]\n","\n","  #Trasformo i valori in numpy.ndarray\n","  train_images = np.array(training_list)\n","  test_images = np.array(test_list)\n","\n","  train_labels = np.array(labels_training_list)\n","  test_labels = np.array(labels_test_list)\n","\n","  train_obf = np.array(obf_training_list)\n","  test_obf = np.array(obf_test_list)\n","\n","  label_encoder = LabelEncoder()\n","  label_encoder.fit(train_labels)\n","  train_labels_encoded = label_encoder.transform(train_labels)\n","  test_labels_encoded = label_encoder.transform(test_labels)\n","\n","\n","  #Normalizzazione valori in range 0-1\n","  train_images = train_images / 65535.0\n","  test_images = test_images / 65535.0\n","\n","  n_img, dim1, dim2 = train_images.shape\n","  n_img2, dim12, dim22 = test_images.shape\n","\n","  train_images = train_images.reshape(n_img, time_steps, n_features, -1, size_ts_blocks)\n","\n","  test_images = test_images.reshape(n_img2, time_steps, n_features, -1, size_ts_blocks)\n","\n","  _, _, _, val_derivato, _ = train_images.shape \n","\n","  #Dichiarazione parametri\n","\n","  n_classes = len(list(label_encoder.classes_))\n","\n","  modelLSTM = ks.Sequential()\n","\n","  #no activation selection\n","\n","  modelLSTM.add(Bidirectional(ConvLSTM2D(num_units1, (3, 3),\n","                              padding='same', unit_forget_bias='true', activation='relu'), \n","                              input_shape=(time_steps, n_features, val_derivato, size_ts_blocks)))\n","  modelLSTM.add(Flatten())\n","  modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","  modelLSTM.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","  #Validation_data è usato al termine di ogni epoch;\n","  #Batch size should be (at most) the same number of hidden cells\n","  es = ks.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n","                                  mode='auto', restore_best_weights=True, verbose=0)\n","  time_callback = TimeHistory()\n","\n","  hist = modelLSTM.fit(train_images, train_labels_encoded, \n","                      batch_size = batch_size,\n","                      validation_data=(test_images, test_labels_encoded), \n","                      epochs=n_epochs, shuffle='true',\n","                      callbacks=[time_callback, es], verbose=0)\n","\n","  number_of_epochs_it_ran = len(hist.history['loss'])\n","\n","  time_per_epoch = time_callback.times\n","  total_time = sum(time_per_epoch)\n","\n","  test_accuracy = modelLSTM.evaluate(test_images, test_labels_encoded)\n","\n","  statistiche_modello = Stats(train_images[0].shape, percentage_required, batch_size, test_accuracy[1], test_accuracy[0], n_epochs, number_of_epochs_it_ran, total_time, \"model2_bi\")\n","\n","  return statistiche_modello"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qILl327lm0y1","colab_type":"text"},"source":["#Struttura risultati\n","LSTM (CuDNN), stesso modello, immagini reshape in 64 e in 256.\n","\n","Bi-LSTM (CuDNN), stesso modello, immagini reshape in 64 e in 256.\n","\n","CNN_LSTM, stesso modello, immagini reshape in 64 e in 256.\n","\n","Bi-CNN_LSTM, stesso modello, immagini reshape in 64 e in 256.\n","\n","non ancora\n","(Ognuna lanciata 3 volte per avere consistenza nei risultati.)\n","\n","**Model1**, (Bi)LSTM (cuDNN):\n","\n","layer CuDNN, 141; dense, n_classes; ts: 64x64\n","\n","**Model2**, (Bi)CNN_LSTM:\n","\n","convLSTM2D, 141, 3x3; flatten; dense, n_classes; ts: 8x8x8 | 8x16x16\n","\n","\n","\n","---\n","\n","\n","\n","**MODEL1**\n","```\n","modelLSTM = ks.Sequential()\n","\n","modelLSTM.add(CuDNNLSTM(num_units1, input_shape=(time_steps, n_features), unit_forget_bias='true'))\n","modelLSTM.add(Dense(n_classes, activation='softmax'))\n","\n","```\n","\n","**MODEL2**\n","```\n","modelLSTM.add(ConvLSTM2D(num_units1, (3, 3), input_shape=(time_steps, n_features, val_derivato, size_ts_blocks), padding='same', unit_forget_bias='true', activation='relu'))\n","modelLSTM.add(Flatten())\n","modelLSTM.add(Dense(n_classes, activation='softmax'))\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"wHTrgVcVDy9k","colab_type":"code","outputId":"a86096b9-c367-43a6-afe2-c518e6770dea","executionInfo":{"status":"ok","timestamp":1586547161907,"user_tz":-120,"elapsed":1585830,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":808}},"source":["lista_res = list()\n","\n","'''\n","codice1 lstm64\n","codice2 lstm256\n","codice3 bilstm64\n","codice4 bilstm256\n","codice5 cnn64\n","codice6 cnn256\n","codice7 bicnn64\n","codice8 bicnn256\n","\n","'''\n","\n","for count in range(1, 9):\n","\n","  if count == 1:\n","    print(\"Sono in count: \" + str(count))\n","    lista_res.append(codice_1())\n","    #lista_res.append(codice_1())\n","    #lista_res.append(codice_1())\n","    \n","  elif count == 2:\n","    print(\"Sono in count: \" + str(count))\n","    lista_res.append(codice_2())\n","    #lista_res.append(codice_2())\n","    #lista_res.append(codice_2())\n","\n","  elif count == 3:\n","    print(\"Sono in count: \" + str(count))\n","    lista_res.append(codice_3())\n","    #lista_res.append(codice_3())\n","    #lista_res.append(codice_3())\n","\n","  elif count == 4:\n","    print(\"Sono in count: \" + str(count))\n","    lista_res.append(codice_4())\n","    #lista_res.append(codice_4())\n","    #lista_res.append(codice_4())\n","\n","  elif count == 5:\n","    test_text = input (\"Are you there?\")\n","    print(\"Sono in count: \" + str(count))\n","    lista_res.append(codice_5())\n","    #lista_res.append(codice_5())\n","    #lista_res.append(codice_5())\n","    \n","  elif count == 6:\n","    print(\"Sono in count: \" + str(count))\n","    lista_res.append(codice_6())\n","    #lista_res.append(codice_6())\n","    #lista_res.append(codice_6())\n","\n","  elif count == 7:\n","    print(\"Sono in count: \" + str(count))\n","    #lista_res.append(codice_7())\n","    #lista_res.append(codice_7())\n","    #lista_res.append(codice_7())\n","\n","  elif count == 8:\n","    print(\"Sono in count: \" + str(count))\n","    #lista_res.append(codice_8())\n","    #lista_res.append(codice_8())\n","    #lista_res.append(codice_8())\n","\n","  else:\n","    print(\"ERROR\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sono in count: 1\n","START IMAGE INPUT\n","END IMAGE INPUT\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","2820/2820 [==============================] - 4s 2ms/sample - loss: 0.9532 - acc: 0.7649\n","Sono in count: 2\n","START IMAGE INPUT\n","END IMAGE INPUT\n","2820/2820 [==============================] - 2s 864us/sample - loss: 0.9143 - acc: 0.8202\n","Sono in count: 3\n","START IMAGE INPUT\n","END IMAGE INPUT\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2820/2820 [==============================] - 0s 156us/sample - loss: 0.5776 - acc: 0.8989\n","Sono in count: 4\n","START IMAGE INPUT\n","END IMAGE INPUT\n","2820/2820 [==============================] - 1s 454us/sample - loss: 0.4645 - acc: 0.9234\n","Are you there?y\n","Sono in count: 5\n","START IMAGE INPUT\n","END IMAGE INPUT\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2820/2820 [==============================] - 2s 601us/sample - loss: 0.5551 - acc: 0.9085\n","Sono in count: 6\n","START IMAGE INPUT\n","END IMAGE INPUT\n","2820/2820 [==============================] - 3s 956us/sample - loss: 0.5018 - acc: 0.9142\n","Sono in count: 7\n","Sono in count: 8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zFs7IpmCQa-I","colab_type":"code","outputId":"c7f294d7-3958-407a-eefc-c81f67ee8c7a","executionInfo":{"status":"ok","timestamp":1586547199546,"user_tz":-120,"elapsed":608,"user":{"displayName":"Pleasant94","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggc18QmWcekRk39ps1vtP2fsFwmCWuEr7kJj8SPVg=s64","userId":"00661494034163855202"}},"colab":{"base_uri":"https://localhost:8080/","height":899}},"source":["for elem in lista_res:\n","  elem.myStats()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dimensione immagini:  (64, 64)\n","Percentuale test set: 20%\n","Dimensione batch size: 512\n","Val accuracy: 0.76489\n","Val loss: 0.95317\n","Epoche di addestramento utilizzate: 51/100\n","Tempo di addestramento: 40.1226 sec\n","Model: model1\n","Dimensione immagini:  (256, 64)\n","Percentuale test set: 20%\n","Dimensione batch size: 256\n","Val accuracy: 0.82021\n","Val loss: 0.91428\n","Epoche di addestramento utilizzate: 56/100\n","Tempo di addestramento: 146.69136 sec\n","Model: model1\n","Dimensione immagini:  (64, 64)\n","Percentuale test set: 20%\n","Dimensione batch size: 256\n","Val accuracy: 0.89894\n","Val loss: 0.57759\n","Epoche di addestramento utilizzate: 55/100\n","Tempo di addestramento: 68.14926 sec\n","Model: model1_bi\n","Dimensione immagini:  (256, 64)\n","Percentuale test set: 20%\n","Dimensione batch size: 256\n","Val accuracy: 0.9234\n","Val loss: 0.46451\n","Epoche di addestramento utilizzate: 59/100\n","Tempo di addestramento: 264.31239 sec\n","Model: model1_bi\n","Dimensione immagini:  (8, 8, 8, 8)\n","Percentuale test set: 20%\n","Dimensione batch size: 512\n","Val accuracy: 0.90851\n","Val loss: 0.55513\n","Epoche di addestramento utilizzate: 37/100\n","Tempo di addestramento: 430.90725 sec\n","Model: model2\n","Dimensione immagini:  (8, 16, 8, 16)\n","Percentuale test set: 20%\n","Dimensione batch size: 256\n","Val accuracy: 0.91418\n","Val loss: 0.50183\n","Epoche di addestramento utilizzate: 16/100\n","Tempo di addestramento: 386.66404 sec\n","Model: model2\n"],"name":"stdout"}]}]}